{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Adaptive Boosting(ADABOOST)** :An algorithm that combines weak classifiers to form a powerful classifier.\n",
    "\n",
    "\n",
    "* The boosting methods should generally be seen as an optimization of residuals.\n",
    "\n",
    "\n",
    "* In the **BAGGING method**, trees are created independently from each other.\n",
    "\n",
    "\n",
    "\n",
    "* In the **ADABOOST method**, trees are created interdependently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaboosting Algorithm Visualization**\n",
    "![alt text](https://miro.medium.com/proxy/1*m2UHkzWWJ0kfQyL5tBFNsQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gradient Boosting Machines (GBM)** : Generalized version of **Adaboost Algoritm** that is easily adapted to classification and regression problems.\n",
    "\n",
    "\n",
    "\n",
    "*  A series of models in the form of a single predictive model is built on the residuals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* GBM creates a series of models in the form of a single predictive model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* A model in the series is created by fitting  on the residuals of a previous model in the series.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* GMB uses the **GRADIENT DESCENT** algorithm that can optimize any loss function that can be derived.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* GMB consists of  **Boosting**  and **Gradient Descent** combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-)MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_model = GradientBoostingClassifier()\n",
    "gbm_model.fit(X_train, y_train)\n",
    "gbm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-)Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gbm_model.predict(X_test)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred) # before model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,  32],\n",
       "       [ 27,  53]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred) # before model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       151\n",
      "           1       0.62      0.66      0.64        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.73      0.72       231\n",
      "weighted avg       0.75      0.74      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred)) # before model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-)Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this section, we will try to determine the optimum **n_estimators, max_depth, learning_rate, min_samples_split**  with the GridSearchCV method.\n",
    "\n",
    "\n",
    "* GridSearchCV: Grid Search Cross Validation Methode\n",
    "\n",
    "\n",
    "\n",
    "* Then , we will create the most optimum model by using optimum **n_estimators, max_depth, learning_rate, min_samples_split**  .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **n_estimators, max_depth, learning_rate, min_samples_split**  are the hyperparameters that we will determine according to ourselves and we want it to be the most optimum.\n",
    "\n",
    "\n",
    "\n",
    "* But instead of relying on our own feeling and sense in order to find the  optimum value of these hyperparameters   , we will find the optimum value of these hyperparameters   by using the gridsearch method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **max_depth**:The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **n_estimators**:  The number of trees in the forest.\n",
    "\n",
    "\n",
    "\n",
    "* **min_samples_split** the minimum number of samples required to split an internal node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " gbm_model.learning_rate# default value of  learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print( gbm_model.max_depth)# default value of max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " gbm_model.min_samples_split# default value of min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " gbm_model.n_estimators#default value of n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n",
    "             \"n_estimators\": [100,500,100],\n",
    "             \"max_depth\": [3,5,10],\n",
    "             \"min_samples_split\": [2,5,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "gbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.001, 0.01, 0.1, 0.05],\n",
       "                         'max_depth': [3, 5, 10],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 500, 100]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1-) Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(learning_rate = 0.1, \n",
    "                                 max_depth = 3,\n",
    "                                min_samples_split = 5,\n",
    "                                n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tuned =  gbm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1 = gbm_tuned.predict(X_test)\n",
    "y_pred1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred1)# after model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118,  33],\n",
       "       [ 26,  54]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred1)# after model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       151\n",
      "           1       0.62      0.68      0.65        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.73      0.72       231\n",
      "weighted avg       0.75      0.74      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred1))# after model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
